{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comp551P3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzfKY_7jNb4u"
      },
      "source": [
        "# Project 3: Multi-label Classification of Image Data (GROUP 65)\n",
        " Kynan Nedellec (260866794), Viet Tran (260924954), Oliver Stappas (260930067)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzNzZy7UuZK1"
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision.models as models\n",
        "import cv2\n",
        "import matplotlib.ticker as mtick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgktLmwUwBeo"
      },
      "source": [
        "#**Helper functions and classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4WTDCYmOZz8"
      },
      "source": [
        "# Preprocessing data\n",
        "class MyCustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels=None, transforms=None):\n",
        "        self.data = data.reshape(data.shape[0], data.shape[1], data.shape[2], 1)\n",
        "        self.labels = labels\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        data = self.data[index]\n",
        "\n",
        "        if transforms is not None:\n",
        "            data = self.transforms(self.data[index])\n",
        "\n",
        "        if self.labels is not None:\n",
        "            return (data, self.labels[index])\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRgTs-mSkpfT"
      },
      "source": [
        "# **Importing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1yHuLx7OYAE"
      },
      "source": [
        "# Importing and Reading files\n",
        "with open(\"images_l.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "with open(\"labels_l.pkl\", \"rb\") as f:\n",
        "    labels = pickle.load(f)\n",
        "with open(\"images_test.pkl\", \"rb\") as f:\n",
        "    test = pickle.load(f)\n",
        "\n",
        "for i in range(9):\n",
        "  # define subplot\n",
        "  plt.subplot(330 + 1 + i)\n",
        "  # plot raw pixel data\n",
        "  plt.imshow(data[i])\n",
        "# show the figure\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pohj7Bqniv4Y"
      },
      "source": [
        "#Converting labels from binary\n",
        "def combine(labels):\n",
        "    new_labels = []\n",
        "    for label in labels:\n",
        "        digits = label[:10]\n",
        "        characters = label[10:]\n",
        "        new_labels.append((np.nonzero(digits)[0] * 26 + np.nonzero(characters)[0])[0])\n",
        "    return new_labels\n",
        "\n",
        "new_labels = combine(labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfbuz08awiaD"
      },
      "source": [
        "####Preprocess and split the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX1yRYV2wbMG"
      },
      "source": [
        "# Splitting dataset\n",
        "train_size = int(0.8 * (len(data)))\n",
        "\n",
        "x_validation = data[train_size:]\n",
        "y_validation = new_labels[train_size:]\n",
        "x_train = data[:train_size]\n",
        "y_train = new_labels[:train_size]\n",
        "\n",
        "train_dataset = MyCustomDataset(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(np.mean(x_train), np.std(x_train)),\n",
        "            torchvision.transforms.RandomAffine(\n",
        "                5, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=0.1\n",
        "            ),\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "validation_dataset = MyCustomDataset(\n",
        "    x_validation,\n",
        "    y_validation,\n",
        "    transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(np.mean(x_validation), np.std(x_validation)),\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "dataset_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset, batch_size=128, shuffle=True\n",
        ")\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    dataset=validation_dataset, batch_size=128, shuffle=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EehDAQnVbbf3"
      },
      "source": [
        "# test data\n",
        "test_dataset = MyCustomDataset(\n",
        "    test,\n",
        "    transforms=transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize(np.mean(test), np.std(test))]\n",
        "    ),\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset, batch_size=128, shuffle=False\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWsoLHy9kxFE"
      },
      "source": [
        "# **Training Methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzMnqHfNW6PH"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F4tFxriXAL_"
      },
      "source": [
        "# train the network\n",
        "def train_model(net, criterion, optimizer, num_epochs):\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "        print(\"Training epoch %d\" % (epoch + 1))\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(dataset_loader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs.float())\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 25 == 24:  # print every 100 mini-batches\n",
        "                print(\"\\t[Minibatches %d] loss: %.4f\" % (i + 1, running_loss / 25))\n",
        "                running_loss = 0.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzRbM54ajMJK"
      },
      "source": [
        "# **Final Model Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv2qqRWElCNJ"
      },
      "source": [
        "## Mnasnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg4t51h8jXHP"
      },
      "source": [
        "# mnasnet0_5\n",
        "mnasnet0_5 = models.mnasnet0_5(num_classes=260, pretrained=False)\n",
        "mnasnet0_5.layers[0] = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "\n",
        "train_model(\n",
        "    mnasnet0_5.to(device),\n",
        "    nn.CrossEntropyLoss(),\n",
        "    optim.Adam(mnasnet0_5.parameters(), lr=0.001, weight_decay=1e-5),\n",
        "    5,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffcSlti5lH5n"
      },
      "source": [
        "## Densenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAvWcnQ9jZGk"
      },
      "source": [
        "# densenet201\n",
        "densenet201 = models.densenet201(num_classes=260, pretrained=False)\n",
        "densenet201.features[0] = nn.Conv2d(\n",
        "    1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
        ")\n",
        "\n",
        "train_model(\n",
        "    densenet201.to(device),\n",
        "    nn.CrossEntropyLoss(),\n",
        "    optim.Adam(densenet201.parameters(), lr=0.001, weight_decay=1e-5),\n",
        "    35,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFPEmEvWlNAR"
      },
      "source": [
        "## Regnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy8X1CAXjj0l"
      },
      "source": [
        "# regnet_y_400mf\n",
        "regnet_y_400mf = models.regnet_y_400mf(num_classes=260, pretrained=False)\n",
        "regnet_y_400mf.stem[0] = nn.Conv2d(\n",
        "    1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
        ")\n",
        "train_model(\n",
        "    regnet_y_400mf.to(device),\n",
        "    nn.CrossEntropyLoss(),\n",
        "    optim.Adam(regnet_y_400mf.parameters(), lr=0.001, weight_decay=1e-5),\n",
        "    50,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlW3bOmVlOsC"
      },
      "source": [
        "## VGG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkzPehCKLIwp"
      },
      "source": [
        "### VGG 19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3IP_Kp7jmnm"
      },
      "source": [
        "vgg19 = models.vgg19_bn(\n",
        "    num_classes=260,\n",
        "    pretrained=False,\n",
        ")\n",
        "vgg19.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "train_model(\n",
        "    vgg19.to(device),\n",
        "    nn.CrossEntropyLoss(),\n",
        "    optim.SGD(vgg19.parameters(), lr=0.01, momentum=0.9),\n",
        "    60,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VabF1vlTLO_R"
      },
      "source": [
        "### VGG 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzkYgYisjtbK"
      },
      "source": [
        "vgg16 = models.vgg16_bn(\n",
        "    num_classes=260,\n",
        "    pretrained=False,\n",
        ")\n",
        "vgg16.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "train_model(\n",
        "    vgg16.to(device),\n",
        "    nn.CrossEntropyLoss(),\n",
        "    optim.SGD(vgg16.parameters(), lr=0.01, momentum=0.9),\n",
        "    60,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaNYucdNT0Sx"
      },
      "source": [
        "### VGG 19 (NO ROTATIONS)\n",
        "* THIS WAS RUN ON A LOADER WITH MINIMAL ROTATIONS, TO INCLUDE IN THE VOTING *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuXhsCphTzxC"
      },
      "source": [
        "vgg19_NO_ROTATIONS = models.vgg19_bn(\n",
        "    num_classes=260,\n",
        "    pretrained=False,\n",
        ")\n",
        "vgg19_NO_ROTATIONS.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "train_model(\n",
        "    vgg19_NO_ROTATIONS.to(device),\n",
        "    nn.CrossEntropyLoss(),\n",
        "    optim.SGD(vgg19_NO_ROTATIONS.parameters(), lr=0.01, momentum=0.9),\n",
        "    60,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxnBD6hAlQVh"
      },
      "source": [
        "## Resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKgg5edFLV59"
      },
      "source": [
        "### Resnet 18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AcU_GyXjv12"
      },
      "source": [
        "# Resnet 18\n",
        "resnet18 = models.resnet18(num_classes=260, pretrained=False)\n",
        "resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "train_model(\n",
        "    resnet18.to(device),\n",
        "    nn.CrossEntropyLoss(),\n",
        "    optim.Adam(resnet18.parameters(), lr=0.001, weight_decay=1e-5),\n",
        "    50,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40KLh-ZZLYhr"
      },
      "source": [
        "### Resnet 152"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1vXpiGDjzw0"
      },
      "source": [
        "# Resnet 152\n",
        "\n",
        "resnet152 = models.resnet152(num_classes=260, pretrained=False)\n",
        "resnet152.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "train_model(\n",
        "    resnet152.to(device),\n",
        "    nn.CrossEntropyLoss(),\n",
        "    optim.Adam(resnet152.parameters(), lr=0.001, weight_decay=1e-5),\n",
        "    50,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqiBiSgglC0F"
      },
      "source": [
        "## Custom Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrn-ULC7W548"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.batch32 = nn.BatchNorm2d(32)\n",
        "        self.batch64 = nn.BatchNorm2d(64)\n",
        "        self.batch128 = nn.BatchNorm2d(128)\n",
        "        self.dropout20 = nn.Dropout(p=0.2)\n",
        "        self.fc1 = nn.Linear(\n",
        "            in_features=3200, out_features=1600\n",
        "        )  # TODO adjust if add conv layers\n",
        "        self.fc2 = nn.Linear(in_features=1600, out_features=800)\n",
        "        self.fc3 = nn.Linear(in_features=800, out_features=260)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.batch32(F.relu(self.conv1(x))))\n",
        "        x = self.pool(self.batch64(F.relu(self.conv2(x))))\n",
        "        x = self.pool(self.batch128(F.relu(self.conv3(x))))\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout20(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout20(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-KEjqiXL5gA"
      },
      "source": [
        "## Training Custom Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSLPza_Uj0Sn"
      },
      "source": [
        "custom_net = Net().to(device)\n",
        "train_model(\n",
        "    custom_net,\n",
        "    nn.CrossEntropyLoss(),\n",
        "    optim.Adam(custom_net.parameters(), lr=0.0005, weight_decay=1e-5),\n",
        "    50,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4icpbDL4lRvc"
      },
      "source": [
        "## Combined Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mWfKV19kcmR"
      },
      "source": [
        "class MyEnsemble(nn.Module):\n",
        "\n",
        "    def __init__(self, modelA, modelB, modelC, modelD, input):\n",
        "        super(MyEnsemble, self).__init__()\n",
        "        self.modelA = modelA\n",
        "        self.modelB = modelB\n",
        "        self.modelC = modelC\n",
        "        self.modelD = modelD\n",
        "        self.modelA.fc = nn.Identity()\n",
        "        self.modelB.fc = nn.Identity()\n",
        "        self.modelC.fc = nn.Identity()\n",
        "        self.modelB.fc3 = nn.Identity()\n",
        "\n",
        "        self.classifier = nn.Linear(1040, input)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.modelA(x.clone())  # clone to make sure x is not changed by inplace methods\n",
        "        x1 = x1.view(x1.size(0), -1)\n",
        "        x2 = self.modelB(x.clone())\n",
        "        x2 = x2.view(x2.size(0), -1)\n",
        "        x3 = self.modelA(x.clone())  # clone to make sure x is not changed by inplace methods\n",
        "        x3 = x3.view(x3.size(0), -1)\n",
        "        x4 = self.modelA(x)  # clone to make sure x is not changed by inplace methods\n",
        "        x4 = x1.view(x4.size(0), -1)\n",
        "        x = torch.cat((x1, x2, x2, x3), dim=1)\n",
        "        \n",
        "        x = self.classifier(F.relu(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dfx7zzDkd6V"
      },
      "source": [
        "models_list = [densenet201, resnet18, vgg16, custom_net]\n",
        "for m in models_list:\n",
        "    for param in m.parameters():\n",
        "        param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwbMTaOZkf7g"
      },
      "source": [
        "model = MyEnsemble(densenet201, resnet18, vgg16, custom_net,260)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PQ0KLo6xRai"
      },
      "source": [
        "# **Model analysis**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY7WRCUN9T8m"
      },
      "source": [
        "def test_accuracy(dataset_loader,net):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataset_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images.float())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoYa2WbQlVB1"
      },
      "source": [
        "# Voting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_E0j7t0j-8h"
      },
      "source": [
        "# Voting\n",
        "def predict_vote(models, loader, test=False):\n",
        "    predictions = []\n",
        "    num_models = len(models)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            \n",
        "            if test:\n",
        "                images = data.to(device)\n",
        "            else:\n",
        "                images, _ = data[0].to(device), data[1].to(device)\n",
        "            batch_prediction = []\n",
        "            \n",
        "            for model in models:   \n",
        "                model.eval()\n",
        "                outputs = model(images.float())\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                batch_prediction.append(torch.Tensor.cpu(predicted).numpy())\n",
        "                \n",
        "            batch_copy = np.zeros(len(batch_prediction[0]))\n",
        "            \n",
        "            for i in range(len(batch_prediction[0])):\n",
        "                index_prediction = []\n",
        "                \n",
        "                for j in range(num_models):\n",
        "                    index_prediction.append(batch_prediction[j][i])\n",
        "                    \n",
        "                values, counts = np.unique(np.array(index_prediction), return_counts=True)\n",
        "                max_index = np.argmax(counts)\n",
        "                \n",
        "                \n",
        "                if len(np.argwhere(counts == np.amax(counts))) > 1:\n",
        "                    for k,_ in enumerate(models):\n",
        "                        print(k, values)\n",
        "                        print(index_prediction)\n",
        "                        if index_prediction[k] in values:\n",
        "                            batch_copy[i] = index_prediction[k]\n",
        "                            break\n",
        "                else:\n",
        "                   batch_copy[i] = values[max_index]\n",
        "                   \n",
        "            predictions.append(batch_copy)\n",
        "    return predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfJmn_SMkCI5"
      },
      "source": [
        "models_list = [\n",
        "    vgg19.to(device),\n",
        "    vgg19_NO_ROTATIONS.to(device),\n",
        "    vgg16.to(device),\n",
        "    resnet152.to(device),\n",
        "    resnet18.to(device),\n",
        "    densenet201.to(device),\n",
        "    resnet18.to(device),\n",
        "    regnet_y_400mf.to(device),\n",
        "    custom_net.to(device),\n",
        "    model.to(device),\n",
        "    mnasnet0_5.to(device),\n",
        "]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqDmES0TkDXV"
      },
      "source": [
        "ensemble_predict = predict_vote(models_list, validation_loader)\n",
        "correct = 0\n",
        "total = 0\n",
        "for i, data in enumerate(validation_loader):\n",
        "    _, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (torch.Tensor(ensemble_predict[i]).cuda() == labels).sum().item()\n",
        "print(correct / total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PHM-qGckUu2"
      },
      "source": [
        "#Decoder\n",
        "def decoder(value):\n",
        "    actual_label = np.zeros(36)\n",
        "    digit = int(value / 26)\n",
        "    character = int(value % 26)\n",
        "    actual_label[digit] = 1\n",
        "    actual_label[character + 10] = 1\n",
        "    return actual_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyFh06CZlePM"
      },
      "source": [
        "# Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7_gArosld0B"
      },
      "source": [
        "predictions = predict_vote(models_list, test_loader, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFcQtC6flW5C"
      },
      "source": [
        "table = [decoder(predict) for predict in np.concatenate(predictions)]\n",
        "\n",
        "dataframe = pd.DataFrame(np.vstack(table))\n",
        "dataframe = dataframe.astype(int)\n",
        "dataframe = dataframe.astype(str)\n",
        "dataframe = pd.DataFrame(pd.Series(dataframe.apply(\"\".join, axis=1)))\n",
        "dataframe.columns = [\"Category\"]\n",
        "dataframe.index.name = \"# Id\"\n",
        "dataframe.to_csv(\"test.csv\", index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS31Z4qNNfDL"
      },
      "source": [
        "## Get training and validation accuracy of all individual models and of bagging "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hQ5G90C0FVl",
        "outputId": "d244037c-0f96-4866-a099-6d6075d1506a"
      },
      "source": [
        "custom = Net()\n",
        "custom.load_state_dict(torch.load(\"./customnet.pth\"))\n",
        "custom.to(device)\n",
        "accuracy = test_accuracy(dataset_loader,custom)\n",
        "name = \"custom\"\n",
        "print(f\"{name} training accuracy: {accuracy}\")\n",
        "accuracy = test_accuracy(validation_loader,custom)\n",
        "print(f\"{name} validation accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom training accuracy: 0.9340416666666667\n",
            "custom validation accuracy: 0.9305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haUniA_oNbqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed896db9-2780-4da7-cf4d-bd4f86db6192"
      },
      "source": [
        "densenet201 = models.densenet201(num_classes=260, pretrained=False)\n",
        "densenet201.features[0] = nn.Conv2d(\n",
        "    1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
        ")\n",
        "densenet201.load_state_dict(torch.load(\"./densenet.pth\"))\n",
        "densenet201.to(device)\n",
        "accuracy = test_accuracy(dataset_loader,densenet201)\n",
        "name = \"densenet201\"\n",
        "print(f\"{name} training accuracy: {accuracy}\")\n",
        "accuracy = test_accuracy(validation_loader,densenet201)\n",
        "print(f\"{name} validation accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "densenet201 training accuracy: 0.9801666666666666\n",
            "densenet201 validation accuracy: 0.9591666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoqrcmWYOeY1",
        "outputId": "0aa40d5e-81c5-4056-8d94-a6860fb25c63"
      },
      "source": [
        "resnet152 = models.resnet152(num_classes=260, pretrained=False)\n",
        "resnet152.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "resnet152.load_state_dict(torch.load(\"./resnet152.pth\"))\n",
        "resnet152.to(device)\n",
        "accuracy = test_accuracy(dataset_loader,resnet152)\n",
        "name = \"resnet152\"\n",
        "print(f\"{name} training accuracy: {accuracy}\")\n",
        "accuracy = test_accuracy(validation_loader,resnet152)\n",
        "print(f\"{name} validation accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet152 training accuracy: 0.96125\n",
            "resnet152 validation accuracy: 0.9585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR9hqtc5SBVq"
      },
      "source": [
        "vgg19 = models.vgg19_bn(\n",
        "    num_classes=260,\n",
        "    pretrained=False,\n",
        ")\n",
        "vgg19.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "vgg19.load_state_dict(torch.load(\"./vgg19_bn.pth\"))\n",
        "vgg19.to(device)\n",
        "accuracy = test_accuracy(dataset_loader,vgg19)\n",
        "name = \"vgg19\"\n",
        "print(f\"{name} training accuracy: {accuracy}\")\n",
        "accuracy = test_accuracy(validation_loader,vgg19)\n",
        "print(f\"{name} validation accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_5bOFfVSYp-"
      },
      "source": [
        "vgg16 = models.vgg16_bn(\n",
        "    num_classes=260,\n",
        "    pretrained=False,\n",
        ")\n",
        "vgg16.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "vgg16.load_state_dict(torch.load(\"./vgg16_bn.pth\"))\n",
        "vgg16.to(device)\n",
        "accuracy = test_accuracy(dataset_loader,vgg16)\n",
        "name = \"vgg16\"\n",
        "print(f\"{name} training accuracy: {accuracy}\")\n",
        "accuracy = test_accuracy(validation_loader,vgg16)\n",
        "print(f\"{name} validation accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puNLvpEL6XAN"
      },
      "source": [
        "model = MyEnsemble(densenet201, resnet18, vgg16, custom_net,260)\n",
        "\n",
        "models_list = [\n",
        "    vgg19.to(device),\n",
        "    vgg16.to(device),\n",
        "    resnet152.to(device),\n",
        "    densenet201.to(device),\n",
        "]\n",
        "\n",
        "ensemble_predict = predict_vote(models_list, dataset_loader)\n",
        "correct = 0\n",
        "total = 0\n",
        "for i, data in enumerate(dataset_loader):\n",
        "    _, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (torch.Tensor(ensemble_predict[i]).cuda() == labels).sum().item()\n",
        "print(f\"ensemble training accuracy: {correct/total}\")\n",
        "\n",
        "ensemble_predict = predict_vote(models_list, validation_loader)\n",
        "correct = 0\n",
        "total = 0\n",
        "for i, data in enumerate(validation_loader):\n",
        "    _, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (torch.Tensor(ensemble_predict[i]).cuda() == labels).sum().item()\n",
        "print(f\"ensemble validation accuracy: {correct/total}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV7i5I2Z1uk2"
      },
      "source": [
        "model_accs = {'Custom Network': (0.968, 0.930),\n",
        "          'ResNet152': (0.988, 0.964),\n",
        "          'DenseNet201': (0.997, 0.968),\n",
        "          'VGG19': (0.989,0.971),\n",
        "          'VGG19 no rotation': (0.99,0.963),\n",
        "          'regnet_y_400mf':(0.971,0.9335),\n",
        "          'VGG16': (0.995,0.971),\n",
        "          'Ensemble': (0.989,0.971)\n",
        "          }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "nh2Kc8nc1bn_",
        "outputId": "381c9b3b-55b3-463a-ac7c-d6aae12c8d55"
      },
      "source": [
        "def model_comparison(models):\n",
        "    names = models.keys()\n",
        "    training_acc = [models[model][0] for model in names]\n",
        "    validation_acc = [models[model][1] for model in names]\n",
        "    fig, ax = plt.subplots(figsize=(12,5))#figsize=(10,5))\n",
        "    # set labels\n",
        "    ax.set_xlabel('Models')\n",
        "    ax.set_ylim(0.9, 1)\n",
        "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1, decimals=2))\n",
        "    ax.set_ylabel(\"Accuracy\")\n",
        "\n",
        "    # plot\n",
        "    plt.suptitle(\"Validation accuracy for each individual model and ensemble model\")\n",
        "    ax.errorbar(names, training_acc, yerr=None, capsize=5, fmt='bo', linestyle='', label='Training')\n",
        "    ax.errorbar(names, validation_acc, yerr=None, capsize=5, fmt='ro', linestyle='', label='Validation')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.plot()\n",
        "model_comparison(model_accs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYeKeXCEWgb2"
      },
      "source": [
        "## Utilizing Unlabeled Data\n",
        "We will predict the labels for the unlabeled data with voting, then train each model on these assigned labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ixnMORWpaH"
      },
      "source": [
        "# train the network\n",
        "def train_model_on_specifit_dataset(net,criterion,optimizer,num_epochs,loader):\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "        print(\"Training epoch %d\" % (epoch+1))\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(loader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "    \n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs.float())\n",
        "        \n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 50 == 49:    # print every 20 mini-batches\n",
        "                print('\\t[Minibatches %d] loss: %.4f' %\n",
        "                    (i + 1, running_loss / 50))\n",
        "                running_loss = 0.0      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPLpB_I_WqAb"
      },
      "source": [
        "# Importing and Reading files\n",
        "with open(\"images_ul.pkl\", \"rb\") as f:\n",
        "    unlabeled_data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUrLnqeZWs67"
      },
      "source": [
        "# 5 SPLITS\n",
        "\n",
        "for i in range(1):\n",
        "    unlabeled_split = unlabeled_data[(i * 6000):(i + 1) * 6000]\n",
        "    \n",
        "    # print(unlabeled_split)\n",
        "    \n",
        "    # PREDICT AND LABEL DATA\n",
        "    cur_dataset = MyCustomDataset(unlabeled_split,transforms=transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(np.mean(unlabeled_split), np.std(unlabeled_split)),\n",
        "            ]\n",
        "        ))\n",
        "    \n",
        "    cur_loader = torch.utils.data.DataLoader(dataset=cur_dataset, batch_size=128,shuffle=False)\n",
        "    predictions = predict_vote(models_list,cur_loader,True)\n",
        "\n",
        "    prediction = [int(p) for p in np.concatenate(predictions)]\n",
        "    \n",
        "    train_dataset = MyCustomDataset(\n",
        "        unlabeled_split,\n",
        "        prediction,\n",
        "        transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(np.mean(unlabeled_split), np.std(unlabeled_split)),\n",
        "                torchvision.transforms.RandomAffine(\n",
        "                    5, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=0.1\n",
        "                ),\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128,shuffle=False)\n",
        "\n",
        "    print(\"Training vgg19_NO_ROTATIONS on subset number \",(i+1))\n",
        "    train_model_on_specifit_dataset(vgg19_NO_ROTATIONS.to(device),nn.CrossEntropyLoss(),optim.SGD(vgg19_NO_ROTATIONS.parameters(), lr=0.0001, momentum=0.9),10, train_loader)\n",
        "\n",
        "    print(\"Training vgg16 on subset number \",(i+1))\n",
        "    train_model_on_specifit_dataset(vgg16.to(device),nn.CrossEntropyLoss(),optim.SGD(vgg16.parameters(), lr=0.0001, momentum=0.9),10, train_loader)\n",
        "\n",
        "    print(\"Training vgg19 on subset number \",(i+1))\n",
        "    train_model_on_specifit_dataset(vgg19.to(device),nn.CrossEntropyLoss(),optim.SGD(vgg19.parameters(), lr=0.0001, momentum=0.9),10, train_loader)\n",
        "\n",
        "    print(\"Training densenet201 on subset number \", (i+1))\n",
        "    train_model_on_specifit_dataset(densenet201.to(device), nn.CrossEntropyLoss(), optim.Adam(densenet201.parameters(), lr=0.00001, weight_decay=1e-5), 10, train_loader)\n",
        "\n",
        "    print(\"Training regnet_y_400mf on subset number \",(i+1))\n",
        "    train_model_on_specifit_dataset(regnet_y_400mf.to(device),nn.CrossEntropyLoss(),optim.Adam(regnet_y_400mf.parameters(), lr=0.0001, weight_decay=1e-5),10, train_loader)\n",
        "    \n",
        "    print(\"Training resnet18 on subset number \",(i+1))\n",
        "    train_model_on_specifit_dataset(resnet18.to(device),nn.CrossEntropyLoss(),optim.Adam(resnet18.parameters(), lr=0.0001, weight_decay=1e-5),10, train_loader)\n",
        "    \n",
        "    print(\"Training resnet152 on subset number \",(i+1))\n",
        "    train_model_on_specifit_dataset(resnet152.to(device),nn.CrossEntropyLoss(),optim.Adam(resnet152.parameters(), lr=0.0001, weight_decay=1e-5),10, train_loader)\n",
        "    \n",
        "    print(\"Training custom_net on subset number \",(i+1))\n",
        "    train_model_on_specifit_dataset(custom_net, nn.CrossEntropyLoss(),optim.Adam(custom_net.parameters(),lr=0.0005,weight_decay=1e-5),10, train_loader)\n",
        "    \n",
        "    print(\"Training model on subset number \",(i+1))\n",
        "    train_model_on_specifit_dataset(model, nn.CrossEntropyLoss(),optim.Adam(model.parameters(),lr=0.0005,weight_decay=1e-5),10, train_loader)\n",
        "\n",
        "    print(\"Training mnasnet0_5 on subset number \",(i+1))\n",
        "    train_model_on_specifit_dataset(mnasnet0_5.to(device),nn.CrossEntropyLoss(),optim.Adam(mnasnet0_5.parameters(), lr=0.001, weight_decay=1e-5),10, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw4eMci4XoqO"
      },
      "source": [
        "## Predicting On Test Set For Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmN2UxmhXtN5"
      },
      "source": [
        "predictions = predict_vote(models_list, test_loader, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4OUy9ryXvKf"
      },
      "source": [
        "table = [decoder(predict) for predict in np.concatenate(predictions)]\n",
        "\n",
        "dataframe = pd.DataFrame(np.vstack(table))\n",
        "dataframe = dataframe.astype(int)\n",
        "dataframe = dataframe.astype(str)\n",
        "dataframe = pd.DataFrame(pd.Series(dataframe.apply(\"\".join, axis=1)))\n",
        "dataframe.columns = [\"Category\"]\n",
        "dataframe.index.name = \"# Id\"\n",
        "dataframe.to_csv(\"test.csv\", index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BcrdRuSWcbk"
      },
      "source": [
        "#**Miscellaneous**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLjUFd3pWpYc"
      },
      "source": [
        "###Compare our model's performance for different data augmentation techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG2FXSW3WhpD"
      },
      "source": [
        "def plot_training_and_validation_on_different_data(title, model, results, data_names, color_names, add_line=None):\n",
        "    fig, ax0 = plt.subplots()\n",
        "\n",
        "    ax0.set_xlabel(\"Number of iterations\")\n",
        "    ax0.set_ylabel(\"Accuracy\")\n",
        "    ax0.tick_params(axis=\"y\", labelcolor=\"b\")\n",
        "    ax0.legend([\"Training accuracy\", \"Validation accuracy\"])\n",
        "\n",
        "    ax0.set_title(title)\n",
        "\n",
        "\n",
        "    for result, data_name, color_name in zip(results, data_names, color_names):\n",
        "        # just plot the training accuracy for the other models \n",
        "        training_acc = result[:, 0]\n",
        "        ax0.plot(range(len(result)), training_acc, color=color_name)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1oIGd27Wr39"
      },
      "source": [
        "###Plot training vs validation accuracy for our custom model, and training accuracies of all other models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8h-b-s-Wxcg"
      },
      "source": [
        "def plot_training_versus_validation(title, models, results, add_line=None):\n",
        "    fig, ax0 = plt.subplots()\n",
        "\n",
        "    ax0.set_xlabel(\"Number of iterations\")\n",
        "    ax0.set_ylabel(\"Accuracy\")\n",
        "    ax0.tick_params(axis=\"y\", labelcolor=\"b\")\n",
        "    ax0.legend([\"Training accuracy\", \"Validation accuracy\"])\n",
        "\n",
        "    ax0.set_title(title)\n",
        "\n",
        "\n",
        "    # plot our custom model a different color than all other models\n",
        "    training_acc = results[0][:, 0]\n",
        "    val_acc = results[0][:, 1]\n",
        "\n",
        "    ax0.plot(range(len(results)), training_acc, color=\"navyblue\")\n",
        "    ax0.plot(range(len(results)), val_acc, color=\"salmon\")\n",
        "\n",
        "\n",
        "    for model, result in zip(models[1:], results[1:]):\n",
        "        # just plot the training accuracy for the other models \n",
        "        training_acc = result[:, 0]\n",
        "        ax0.plot(range(len(result)), training_acc, color=\"khaki\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI3bNLk6W7LE"
      },
      "source": [
        "#### Custom transformation class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcmK-SOKW6DV"
      },
      "source": [
        "class CustomTransformations(object):\n",
        "    def __init__(self, draw_box=False, blur=False, denoise=False, angle=0):\n",
        "      self.draw_box = draw_box\n",
        "      self.blur = blur\n",
        "      self.denoise = denoise\n",
        "      self.angle = angle\n",
        "\n",
        "    # draw bounding boxes around the large features\n",
        "    def __call__(self, img):\n",
        "        img = img.astype(np.float64)\n",
        "        img = img[:,:]\n",
        "        img = self.rotate_image(img)\n",
        "        if self.blur:\n",
        "            img = self.blurImage(img)\n",
        "        if self.denoise:\n",
        "            img = self.removeNoise(img)\n",
        "        if self.draw_box:\n",
        "            img = self.drawBox(img)\n",
        "        return img.astype(np.float64)\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__+'()'\n",
        "\n",
        "    def drawBox(self,img):\n",
        "        contours, hierarchy  = cv2.findContours(img, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        for c in contours:\n",
        "            x,y,w,h = cv2.boundingRect(c)\n",
        "            if (w >= 10 and w <= 50) and (h >= 10 and h <= 50):\n",
        "                img = cv2.rectangle(img,(x,y),(x+w,y+h),(150),1)\n",
        "        return img\n",
        "\n",
        "    def removeNoise(self,img, threshold = 7):\n",
        "        img[img < threshold] = 0\n",
        "        return img\n",
        "\n",
        "    def blurImage(self,img, sigma=2):\n",
        "        img = cv2.resize(img, dsize=(100, 100), interpolation=cv2.INTER_CUBIC)\n",
        "        img = cv2.GaussianBlur(img, (3,3), 2)\n",
        "        img = cv2.resize(img, dsize=(56, 56), interpolation=cv2.INTER_CUBIC)\n",
        "        return img\n",
        "\n",
        "    def rotate_image(self,mat):\n",
        "\n",
        "        # Source: https://stackoverflow.com/questions/43892506/opencv-python-rotate-image-without-cropping-sides\n",
        "        height, width = mat.shape[:2] # image shape has 3 dimensions\n",
        "        image_center = (width/2, height/2) # getRotationMatrix2D needs coordinates in reverse order (width, height) compared to shape\n",
        "        rotation_mat = cv2.getRotationMatrix2D(image_center, self.angle, 1.)\n",
        "\n",
        "        # rotation calculates the cos and sin, taking absolutes of those.\n",
        "        abs_cos = abs(rotation_mat[0,0]) \n",
        "        abs_sin = abs(rotation_mat[0,1])\n",
        "\n",
        "        # find the new width and height bounds\n",
        "        bound_w = int(height * abs_sin + width * abs_cos)\n",
        "        bound_h = int(height * abs_cos + width * abs_sin)\n",
        "\n",
        "        # # subtract old image center (bringing image back to origo) and adding the new image center coordinates\n",
        "        rotation_mat[0, 2] += bound_w/2 - image_center[0]\n",
        "        rotation_mat[1, 2] += bound_h/2 - image_center[1]\n",
        "\n",
        "        # rotate image with the new bounds and translated rotation matrix\n",
        "        rotated_mat = cv2.warpAffine(mat, rotation_mat, (bound_w, bound_h))\n",
        "        return cv2.resize(rotated_mat, dsize=(56, 56), interpolation=cv2.INTER_CUBIC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ZNics9XC2Y"
      },
      "source": [
        "#### Adding the augmented data to the original data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybMkxY_OXBp5"
      },
      "source": [
        "# data_augmented1 = np.copy(data)\n",
        "# data_augmented2 = np.copy(data)\n",
        "# data_augmented3 = np.copy(data)\n",
        "\n",
        "# ct1 = CustomTransformations(draw_box=False, blur=False, denoise=False, angle=20)\n",
        "# ct2 = CustomTransformations(draw_box=False, blur=False, denoise=False, angle=-20)\n",
        "# ct3 = CustomTransformations(draw_box=False, blur=False, denoise=False, angle=-20)\n",
        "\n",
        "\n",
        "# for i, im in enumerate(data_augmented1):\n",
        "#     data_augmented1[i] = ct1.__call__(im)\n",
        "#     data_augmented2[i] = ct2.__call__(im)\n",
        "#     data_augmented3[i] = ct3.__call__(im)\n",
        "\n",
        "# # merge with original data and shuffle\n",
        "# data = np.vstack((data,data_augmented1, data_augmented2, data_augmented3))\n",
        "# labels = np.vstack((labels, labels.copy(), labels.copy(), labels.copy()))\n",
        "# print(data.shape)\n",
        "# print(labels.shape)\n",
        "\n",
        "# np.random.seed(1)\n",
        "# np.random.shuffle(data)\n",
        "# np.random.seed(1)\n",
        "# np.random.shuffle(labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}